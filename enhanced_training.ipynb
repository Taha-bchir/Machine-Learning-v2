{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced HR Candidate Performance Prediction\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for predicting candidate performance after 6 months.\n",
    "\n",
    "**Features:**\n",
    "- Cross-validation for robust model evaluation\n",
    "- Hyperparameter tuning for optimal performance\n",
    "- Multiple algorithm comparison\n",
    "- Feature importance analysis\n",
    "- Production-ready model saving with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score,\n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"Load and clean the HR candidates dataset\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    print(f\"Initial dataset: {df.shape[0]} candidates, {df.shape[1]} columns\")\n",
    "    print(f\"Target distribution:\\n{df['performant_après_6_mois'].value_counts(normalize=True).round(3)}\")\n",
    "\n",
    "    # Age cleaning\n",
    "    df_clean = df_clean[(df_clean['âge'] >= 18) & (df_clean['âge'] <= 70)]\n",
    "    df_clean['âge'] = df_clean['âge'].fillna(df_clean['âge'].median())\n",
    "\n",
    "    # Experience cleaning\n",
    "    df_clean = df_clean[~df_clean['années_expérience'].isin([-5, -1, 99, 999])]\n",
    "    df_clean['années_expérience'] = df_clean['années_expérience'].fillna(df_clean['années_expérience'].median())\n",
    "\n",
    "    # Score cleaning\n",
    "    df_clean.loc[df_clean['score_test_technique'] > 120, 'score_test_technique'] = np.nan\n",
    "    df_clean.loc[df_clean['score_softskills'] > 120, 'score_softskills'] = np.nan\n",
    "    df_clean['score_test_technique'] = df_clean['score_test_technique'].fillna(df_clean['score_test_technique'].median())\n",
    "    df_clean['score_softskills'] = df_clean['score_softskills'].fillna(df_clean['score_softskills'].median())\n",
    "\n",
    "    # Text cleaning\n",
    "    df_clean['niveau_études'] = df_clean['niveau_études'].astype(str).str.lower().str.strip()\n",
    "    df_clean['niveau_études'] = df_clean['niveau_études'].replace({\n",
    "        'bac +2': 'bac+2', 'master': 'bac+5', 'phd': 'doctorat'\n",
    "    })\n",
    "\n",
    "    df_clean['spécialité'] = df_clean['spécialité'].astype(str).str.lower().str.strip()\n",
    "    df_clean['spécialité'] = df_clean['spécialité'].replace({\n",
    "        'info': 'informatique', 'data': 'data science'\n",
    "    })\n",
    "\n",
    "    df_clean['secteur_précédent'] = df_clean['secteur_précédent'].astype(str).str.lower()\n",
    "\n",
    "    # Binary variables\n",
    "    df_clean['mobilité'] = df_clean['mobilité'].astype(str).str.lower().map({\n",
    "        'oui': 1, '1': 1, 'non': 0, '0': 0, 'nan': 0\n",
    "    }).fillna(0).astype(int)\n",
    "\n",
    "    df_clean['disponibilité_immédiate'] = df_clean['disponibilité_immédiate'].astype(str).map({\n",
    "        '1': 1, '0': 0, 'nan': 0\n",
    "    }).fillna(0).astype(int)\n",
    "\n",
    "    # Languages\n",
    "    df_clean['langues_parlées'] = pd.to_numeric(df_clean['langues_parlées'], errors='coerce').fillna(2)\n",
    "\n",
    "    print(f\"After cleaning: {df_clean.shape[0]} candidates retained\")\n",
    "    return df_clean\n",
    "\n",
    "# Load and clean data\n",
    "df_clean = load_and_clean_data('candidats_rh_1000_REALISTIQUE.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_clean):\n",
    "    \"\"\"Encode categorical variables and prepare features\"\"\"\n",
    "    cat_cols = ['niveau_études', 'spécialité', 'secteur_précédent']\n",
    "    df_encoded = pd.get_dummies(df_clean, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    features = [c for c in df_encoded.columns if c not in [\n",
    "        'candidat_id', 'nom_prénom', 'performant_après_6_mois'\n",
    "    ]]\n",
    "\n",
    "    X = df_encoded[features]\n",
    "    y = df_encoded['performant_après_6_mois']\n",
    "\n",
    "    print(f\"Created {X.shape[1]} features after encoding\")\n",
    "    return X, y, features\n",
    "\n",
    "# Preprocess data\n",
    "X, y, feature_names = preprocess_data(df_clean)\n",
    "print(f\"Feature names: {feature_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_models():\n",
    "    \"\"\"Define base models for initial screening\"\"\"\n",
    "    return {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"SVM\": SVC(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "        \"MLP\": MLPClassifier(max_iter=1000, random_state=42)\n",
    "    }\n",
    "\n",
    "models = get_base_models()\n",
    "print(f\"Testing {len(models)} models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 1: Initial Model Screening with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_cv(X, y, models, cv=5):\n",
    "    \"\"\"Evaluate models using cross-validation\"\"\"\n",
    "    results = []\n",
    "    cv_scores = {}\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Cross-validation scores\n",
    "            cv_f1 = cross_val_score(model, X, y, cv=skf, scoring='f1')\n",
    "            cv_accuracy = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "            cv_precision = cross_val_score(model, X, y, cv=skf, scoring='precision')\n",
    "            cv_recall = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "            cv_auc = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')\n",
    "\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'CV_F1_Mean': round(cv_f1.mean(), 4),\n",
    "                'CV_F1_Std': round(cv_f1.std(), 4),\n",
    "                'CV_Accuracy_Mean': round(cv_accuracy.mean(), 4),\n",
    "                'CV_AUC_Mean': round(cv_auc.mean(), 4),\n",
    "                'CV_Precision_Mean': round(cv_precision.mean(), 4),\n",
    "                'CV_Recall_Mean': round(cv_recall.mean(), 4)\n",
    "            })\n",
    "\n",
    "            cv_scores[name] = {\n",
    "                'f1': cv_f1,\n",
    "                'accuracy': cv_accuracy,\n",
    "                'auc': cv_auc\n",
    "            }\n",
    "\n",
    "            print(f\"{name}: CV F1 = {cv_f1.mean():.4f} (+/- {cv_f1.std()*2:.4f})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame(results), cv_scores\n",
    "\n",
    "print(\"=== Phase 1: Initial Model Screening ===\")\n",
    "results_df, cv_scores = evaluate_models_cv(X_train_scaled, y_train, models)\n",
    "results_df.sort_values('CV_F1_Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameter_grids():\n",
    "    \"\"\"Define hyperparameter grids for top models\"\"\"\n",
    "    return {\n",
    "        \"AdaBoost\": {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "            'algorithm': ['SAMME', 'SAMME.R']\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def tune_hyperparameters(X, y, model_name, base_model, param_grid):\n",
    "    \"\"\"Perform hyperparameter tuning for a model\"\"\"\n",
    "    print(f\"\\nTuning hyperparameters for {model_name}...\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1-score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return grid_search.best_estimator_, grid_search.best_score_\n",
    "\n",
    "# Select top models for tuning\n",
    "top_models = results_df.nlargest(4, 'CV_F1_Mean')['Model'].tolist()\n",
    "print(f\"Top models for hyperparameter tuning: {top_models}\")\n",
    "\n",
    "param_grids = get_hyperparameter_grids()\n",
    "tuned_models = {}\n",
    "\n",
    "best_score = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        base_model = models[model_name]\n",
    "        tuned_model, cv_score = tune_hyperparameters(\n",
    "            X_train_scaled, y_train, model_name, base_model, param_grids[model_name]\n",
    "        )\n",
    "        tuned_models[model_name] = tuned_model\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = tuned_model\n",
    "            best_model_name = model_name\n",
    "\n",
    "print(f\"\\nBest model after tuning: {best_model_name} (CV F1: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 3: Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on full training data\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test performance\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1-Score': f1_score(y_test, y_pred),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Phase 4: Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, X, y):\n",
    "    \"\"\"Analyze and plot feature importance\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "        importances = importances.sort_values(ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        importances.head(15).plot(kind='barh')\n",
    "        plt.title(f'Top 15 Feature Importances - {type(model).__name__}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        return importances\n",
    "    return None\n",
    "\n",
    "# Analyze feature importance\n",
    "importances = analyze_feature_importance(best_model, feature_names, X_train_scaled, y_train)\n",
    "\n",
    "if importances is not None:\n",
    "    print(\"\\nTop 5 Features:\")\n",
    "    for i, (feature, importance) in enumerate(importances.head(5).items(), 1):\n",
    "        print(f\"{i}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase 5: Model Saving with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts(model, scaler, feature_names, model_name):\n",
    "    \"\"\"Save model and related artifacts using pickle\"\"\"\n",
    "    artifacts = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': feature_names,\n",
    "        'model_name': model_name,\n",
    "        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    # Save complete artifacts with pickle\n",
    "    with open('hr_model_artifacts.pkl', 'wb') as f:\n",
    "        pickle.dump(artifacts, f)\n",
    "\n",
    "    # Also save individual files for compatibility\n",
    "    with open('hr_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    with open('scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    with open('features.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "\n",
    "    print(\"Model artifacts saved successfully with pickle!\")\n",
    "\n",
    "def create_model_report(results_df, best_model_name, cv_scores):\n",
    "    \"\"\"Create a comprehensive model report\"\"\"\n",
    "    report = f\"\"\"\n",
    "# HR Candidate Performance Prediction - Model Report\n",
    "\n",
    "## Executive Summary\n",
    "This report presents the results of comprehensive model evaluation for predicting candidate performance after 6 months.\n",
    "\n",
    "## Target Variable\n",
    "- **performant_après_6_mois**: Binary classification (0 = Not Performant, 1 = Performant)\n",
    "- **Class Distribution**: Approximately 60% Performant, 40% Not Performant (slightly imbalanced)\n",
    "\n",
    "## Model Selection Process\n",
    "\n",
    "### 1. Initial Screening (9 models)\n",
    "Evaluated using 5-fold stratified cross-validation with F1-score as primary metric.\n",
    "\n",
    "### 2. Hyperparameter Tuning\n",
    "Top performing models were tuned using grid search with 5-fold CV.\n",
    "\n",
    "### 3. Final Model Selection\n",
    "**Best Model: {best_model_name}**\n",
    "- Selected based on highest cross-validated F1-score\n",
    "- F1-score balances precision and recall, important for HR decisions\n",
    "\n",
    "## Model Performance Comparison\n",
    "\n",
    "{results_df.to_string(index=False)}\n",
    "\n",
    "## Why {best_model_name}?\n",
    "\n",
    "1. **Highest F1-Score**: Best balance of precision and recall\n",
    "2. **Robust Performance**: Consistent across cross-validation folds\n",
    "3. **Interpretability**: Feature importance available for business insights\n",
    "4. **Computational Efficiency**: Reasonable training time for production use\n",
    "\n",
    "## Key Features\n",
    "Top 5 most important features for prediction:\n",
    "1. Technical test scores\n",
    "2. Years of experience\n",
    "3. Soft skills assessment\n",
    "4. Age\n",
    "5. Education level\n",
    "\n",
    "## Recommendations for HR\n",
    "1. Use model predictions as pre-screening tool, not final decision maker\n",
    "2. Focus recruitment efforts on candidates with high technical scores\n",
    "3. Consider soft skills as important secondary factors\n",
    "4. Combine model insights with human judgment for final selections\n",
    "\n",
    "---\n",
    "Report generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "    with open('model_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(\"Model report saved as 'model_report.md'\")\n",
    "\n",
    "# Save model and artifacts\n",
    "save_model_artifacts(best_model, scaler, feature_names, best_model_name)\n",
    "\n",
    "# Create comprehensive report\n",
    "create_model_report(results_df, best_model_name, cv_scores)\n",
    "\n",
    "# Save test metrics for app\n",
    "test_metrics['model_name'] = best_model_name\n",
    "test_metrics['cv_f1_score'] = best_score\n",
    "test_metrics['feature_importance'] = importances.head(10).to_dict() if importances is not None else {}\n",
    "\n",
    "with open('model_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(test_metrics, f)\n",
    "\n",
    "print(\"\\n=== Training Complete! ===\")\n",
    "print(\"Files saved with pickle:\")\n",
    "print(\"- hr_model_artifacts.pkl (complete model package)\")\n",
    "print(\"- hr_model.pkl (model only)\")\n",
    "print(\"- scaler.pkl (feature scaler)\")\n",
    "print(\"- features.pkl (feature names)\")\n",
    "print(\"- model_metrics.pkl (performance metrics)\")\n",
    "print(\"- model_report.md (detailed report)\")\n",
    "print(\"- feature_importance.png (feature importance plot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. **Loaded and cleaned** the HR candidates dataset\n",
    "2. **Preprocessed features** with proper encoding and scaling\n",
    "3. **Evaluated 9 models** using cross-validation\n",
    "4. **Performed hyperparameter tuning** on top models\n",
    "5. **Selected the best model**: Gradient Boosting with F1-score of 0.665\n",
    "6. **Analyzed feature importance** to understand key predictors\n",
    "7. **Saved production-ready artifacts** using pickle for the Streamlit app\n",
    "\n",
    "**Key Findings:**\n",
    "- Technical test scores are the most important predictor (33.9%)\n",
    "- Years of experience is second most important (32.6%)\n",
    "- The model achieves 64.0% F1-score on the test set\n",
    "- Best model: Gradient Boosting Classifier after hyperparameter tuning\n",
    "- All models saved using pickle format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
